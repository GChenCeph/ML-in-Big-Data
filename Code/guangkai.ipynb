{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jdcal\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import jdcal\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "state_to_id = {\n",
    "    \"AL\": 1, \"AZ\": 2, \"AR\": 3, \"CA\": 4, \"CO\": 5,\n",
    "    \"CT\": 6, \"DE\": 7, \"FL\": 8, \"GA\": 9, \"ID\": 10,\n",
    "    \"IL\": 11, \"IN\": 12, \"IA\": 13, \"KS\": 14, \"KY\": 15,\n",
    "    \"LA\": 16, \"ME\": 17, \"MD\": 18, \"MA\": 19, \"MI\": 20,\n",
    "    \"MN\": 21, \"MS\": 22, \"MO\": 23, \"MT\": 24, \"NE\": 25,\n",
    "    \"NV\": 26, \"NH\": 27, \"NJ\": 28, \"NM\": 29, \"NY\": 30,\n",
    "    \"NC\": 31, \"ND\": 32, \"OH\": 33, \"OK\": 34, \"OR\": 35,\n",
    "    \"PA\": 36, \"RI\": 37, \"SC\": 38, \"SD\": 39, \"TN\": 40,\n",
    "    \"TX\": 41, \"UT\": 42, \"VT\": 43, \"VA\": 44, \"WA\": 45,\n",
    "    \"WV\": 46, \"WI\": 47, \"WY\": 48, \"HI\": 49, \"AK\": 50\n",
    "}\n",
    "\n",
    "\n",
    "def julian_to_gregorian(julian_date):\n",
    "    year, month, day, frac = jdcal.jd2gcal(jdcal.MJD_0, julian_date - jdcal.MJD_0)\n",
    "    return datetime(year, month, day)\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(\"../Data/FPA_FOD_20170508.sqlite\")\n",
    "df = pd.read_sql_query(\"SELECT STATE, DISCOVERY_DATE FROM Fires\", conn) #\"SELECT * FROM 'Fires'\",conn\n",
    "\n",
    "df['DISCOVERY_DATE'] = df['DISCOVERY_DATE'].apply(julian_to_gregorian)\n",
    "\n",
    "df['year'] = df['DISCOVERY_DATE'].dt.year\n",
    "df['month'] = df['DISCOVERY_DATE'].dt.month\n",
    "\n",
    "df = df.drop('DISCOVERY_DATE', axis=1)\n",
    "\n",
    "df['STATE'] = df['STATE'].map(state_to_id)\n",
    "\n",
    "df = df.dropna(subset=['STATE'])\n",
    "\n",
    "df['STATE'] = df['STATE'].astype(int)\n",
    "\n",
    "# Rename the STATE column to state\n",
    "df.rename(columns={'STATE': 'state'}, inplace=True)\n",
    "\n",
    "df.to_csv(\"../Data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define state codes and month codes\n",
    "state_codes = {\n",
    "    \"Alabama\": 1, \"Arizona\": 2, \"Arkansas\": 3, \"California\": 4, \"Colorado\": 5,\n",
    "    \"Connecticut\": 6, \"Delaware\": 7, \"Florida\": 8, \"Georgia\": 9, \"Idaho\": 10,\n",
    "    \"Illinois\": 11, \"Indiana\": 12, \"Iowa\": 13, \"Kansas\": 14, \"Kentucky\": 15,\n",
    "    \"Louisiana\": 16, \"Maine\": 17, \"Maryland\": 18, \"Massachusetts\": 19, \"Michigan\": 20,\n",
    "    \"Minnesota\": 21, \"Mississippi\": 22, \"Missouri\": 23, \"Montana\": 24, \"Nebraska\": 25,\n",
    "    \"Nevada\": 26, \"New Hampshire\": 27, \"New Jersey\": 28, \"New Mexico\": 29, \"New York\": 30,\n",
    "    \"North Carolina\": 31, \"North Dakota\": 32, \"Ohio\": 33, \"Oklahoma\": 34, \"Oregon\": 35,\n",
    "    \"Pennsylvania\": 36, \"Rhode Island\": 37, \"South Carolina\": 38, \"South Dakota\": 39,\n",
    "    \"Tennessee\": 40, \"Texas\": 41, \"Utah\": 42, \"Vermont\": 43, \"Virginia\": 44,\n",
    "    \"Washington\": 45, \"West Virginia\": 46, \"Wisconsin\": 47, \"Wyoming\": 48, \"Alaska\": 50\n",
    "}\n",
    "\n",
    "month_codes = {\n",
    "    \"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4, \"May\": 5, \"June\": 6, \n",
    "    \"July\": 7, \"August\": 8, \"September\": 9, \"October\": 10, \"November\": 11, \"December\": 12\n",
    "}\n",
    "\n",
    "# Function to categorize fire count\n",
    "def categorize_fire_count(fire_count):\n",
    "    if fire_count <= 3:\n",
    "        return 1\n",
    "    elif fire_count <= 29:\n",
    "        return 2\n",
    "    elif fire_count <= 141:\n",
    "        return 3\n",
    "    elif fire_count <= 348:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Load the wildfire data\n",
    "wilds = pd.read_csv('../Data/data.csv')\n",
    "\n",
    "# Preprocess to create a summary of fire counts per state and year for each month\n",
    "fire_summary = wilds.groupby(['state', 'year', 'month']).size().reset_index(name='count')\n",
    "fire_summary_dict = {(row['state'], row['year'], row['month']): row['count'] for _, row in fire_summary.iterrows()}\n",
    "\n",
    "# File paths of datasets to be processed\n",
    "file_paths = [\n",
    "    'climdiv-pcpnst-v1.0.0-20231106.txt', 'climdiv-pdsist-v1.0.0-20231106.txt',\n",
    "    'climdiv-phdist-v1.0.0-20231106.txt', 'climdiv-sp01st-v1.0.0-20231106.txt',\n",
    "    'climdiv-sp02st-v1.0.0-20231106.txt', 'climdiv-sp06st-v1.0.0-20231106.txt',\n",
    "    'climdiv-sp12st-v1.0.0-20231106.txt', 'climdiv-sp24st-v1.0.0-20231106.txt',\n",
    "    'climdiv-tmaxst-v1.0.0-20231106.txt', 'climdiv-tminst-v1.0.0-20231106.txt',\n",
    "    'climdiv-tmpcst-v1.0.0-20231106.txt', 'climdiv-zndxst-v1.0.0-20231106.txt'\n",
    "]\n",
    "\n",
    "# Initialize DataFrame to store the combined data\n",
    "combined_df = None\n",
    "\n",
    "# Process each file and merge into combined_df\n",
    "for file_path in file_paths:\n",
    "    data = []\n",
    "    with open('../Data/'+file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            state_id = int(words[0][:3])\n",
    "            year_val = int(words[0][-4:])\n",
    "            monthly_data = [float(word) for word in words[1:13]]\n",
    "            data.append([state_id, year_val] + monthly_data)\n",
    "\n",
    "    name = file_path.split('/')[-1].split('-')[1]\n",
    "    columns = ['state', 'year'] + [f'{name}_m{i}' for i in range(1, 13)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    if combined_df is None:\n",
    "        combined_df = df\n",
    "    else:\n",
    "        combined_df = pd.merge(combined_df, df, on=['state', 'year'])\n",
    "\n",
    "# Categorize fire counts for each month\n",
    "for month in range(1, 13):\n",
    "    combined_df[f'f{month}'] = combined_df.apply(\n",
    "        lambda row: categorize_fire_count(\n",
    "            fire_summary_dict.get((row['state'], row['year'], month), 0)\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "# Split the dataset into training, reference, and testing datasets\n",
    "training_df = combined_df[(combined_df['year'] >= 1992) & (combined_df['year'] <= 2013) & (combined_df['state'] < 49)]\n",
    "reference = combined_df[(combined_df['year'] >= 2014) & (combined_df['year'] <= 2015) & (combined_df['state'] < 49)]\n",
    "testing_df = reference.copy()\n",
    "\n",
    "# Set fire counts to 0 in the testing dataset\n",
    "for i in range(1, 13):\n",
    "    testing_df[f'f{i}'] = 0\n",
    "\n",
    "# Save the datasets\n",
    "training_df.to_csv('../Data/training.csv', index=False)\n",
    "reference.to_csv('../Data/reference.csv', index=False)\n",
    "testing_df.to_csv('../Data/testing.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points in testing set: 96\n",
      "\n",
      "Number of anomalies detected: 39\n",
      "\n",
      "\n",
      "Anomaly Detection Comparison with Reference Data:\n",
      "\n",
      "Month 1 - Detected Anomalies: 0, Actual Fires: 33\n",
      "\n",
      "Month 2 - Detected Anomalies: 0, Actual Fires: 31\n",
      "\n",
      "Month 3 - Detected Anomalies: 0, Actual Fires: 15\n",
      "\n",
      "Month 4 - Detected Anomalies: 0, Actual Fires: 3\n",
      "\n",
      "Month 5 - Detected Anomalies: 0, Actual Fires: 8\n",
      "\n",
      "Month 6 - Detected Anomalies: 0, Actual Fires: 14\n",
      "\n",
      "Month 7 - Detected Anomalies: 0, Actual Fires: 20\n",
      "\n",
      "Month 8 - Detected Anomalies: 0, Actual Fires: 19\n",
      "\n",
      "Month 9 - Detected Anomalies: 0, Actual Fires: 16\n",
      "\n",
      "Month 10 - Detected Anomalies: 0, Actual Fires: 13\n",
      "\n",
      "Month 11 - Detected Anomalies: 0, Actual Fires: 15\n",
      "\n",
      "Month 12 - Detected Anomalies: 0, Actual Fires: 43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv('../Data/training.csv')\n",
    "test_df = pd.read_csv('../Data/testing.csv')\n",
    "reference_df = pd.read_csv('../Data/reference.csv')\n",
    "\n",
    "X_train = train_df.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "X_test = test_df.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "y_reference = reference_df[[f'f{i}' for i in range(1, 13)]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.5, random_state=42)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "\n",
    "anomalies = iso_forest.predict(X_test_scaled)\n",
    "\n",
    "test_df['anomaly'] = anomalies\n",
    "\n",
    "anomalous_data = test_df[test_df['anomaly'] == -1]\n",
    "\n",
    "\n",
    "print(f\"Total data points in testing set: {len(test_df)}\\n\")\n",
    "print(f\"Number of anomalies detected: {len(anomalous_data)}\\n\")\n",
    "\n",
    "print(\"\\nAnomaly Detection Comparison with Reference Data:\\n\")\n",
    "for i in range(1, 13):\n",
    "    anomaly_count = anomalous_data[anomalous_data[f'f{i}'] == 1].shape[0]\n",
    "    actual_fire_count = y_reference[y_reference[f'f{i}'] == 1].shape[0]\n",
    "    print(f\"Month {i} - Detected Anomalies: {anomaly_count}, Actual Fires: {actual_fire_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for each month:\n",
      "\n",
      "\n",
      "Month 1 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      1.00      0.66        33\n",
      "           2       0.50      0.07      0.13        27\n",
      "           3       0.19      0.25      0.21        12\n",
      "           4       0.62      0.31      0.42        16\n",
      "           5       1.00      0.12      0.22         8\n",
      "\n",
      "    accuracy                           0.46        96\n",
      "   macro avg       0.56      0.35      0.33        96\n",
      "weighted avg       0.52      0.46      0.38        96\n",
      "\n",
      "Accuracy for Month 1: 0.4583333333333333\n",
      "\n",
      "\n",
      "Month 2 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.97      0.71        31\n",
      "           2       0.73      0.31      0.43        26\n",
      "           3       0.25      0.31      0.28        13\n",
      "           4       0.67      0.24      0.35        17\n",
      "           5       0.50      0.56      0.53         9\n",
      "\n",
      "    accuracy                           0.53        96\n",
      "   macro avg       0.54      0.47      0.46        96\n",
      "weighted avg       0.58      0.53      0.50        96\n",
      "\n",
      "Accuracy for Month 2: 0.53125\n",
      "\n",
      "\n",
      "Month 3 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.67      0.61        15\n",
      "           2       0.47      0.84      0.60        19\n",
      "           3       0.23      0.23      0.23        22\n",
      "           4       1.00      0.04      0.07        27\n",
      "           5       0.38      0.62      0.47        13\n",
      "\n",
      "    accuracy                           0.42        96\n",
      "   macro avg       0.53      0.48      0.40        96\n",
      "weighted avg       0.56      0.42      0.35        96\n",
      "\n",
      "Accuracy for Month 3: 0.4166666666666667\n",
      "\n",
      "\n",
      "Month 4 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.33      0.22         3\n",
      "           2       0.35      0.57      0.43        14\n",
      "           3       0.69      0.59      0.64        37\n",
      "           4       0.39      0.28      0.33        25\n",
      "           5       0.41      0.41      0.41        17\n",
      "\n",
      "    accuracy                           0.47        96\n",
      "   macro avg       0.40      0.44      0.41        96\n",
      "weighted avg       0.50      0.47      0.47        96\n",
      "\n",
      "Accuracy for Month 4: 0.46875\n",
      "\n",
      "\n",
      "Month 5 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.75      0.38         8\n",
      "           2       0.12      0.13      0.12        15\n",
      "           3       0.69      0.62      0.65        50\n",
      "           4       0.33      0.08      0.13        12\n",
      "           5       0.86      0.55      0.67        11\n",
      "\n",
      "    accuracy                           0.48        96\n",
      "   macro avg       0.45      0.43      0.39        96\n",
      "weighted avg       0.54      0.48      0.48        96\n",
      "\n",
      "Accuracy for Month 5: 0.4791666666666667\n",
      "\n",
      "\n",
      "Month 6 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.93      0.58        14\n",
      "           2       0.87      0.34      0.49        38\n",
      "           3       0.48      0.76      0.59        21\n",
      "           4       0.67      0.57      0.62        14\n",
      "           5       1.00      0.56      0.71         9\n",
      "\n",
      "    accuracy                           0.57        96\n",
      "   macro avg       0.69      0.63      0.60        96\n",
      "weighted avg       0.70      0.57      0.56        96\n",
      "\n",
      "Accuracy for Month 6: 0.5729166666666666\n",
      "\n",
      "\n",
      "Month 7 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.85      0.69        20\n",
      "           2       0.67      0.50      0.57        20\n",
      "           3       0.61      0.74      0.67        23\n",
      "           4       0.62      0.28      0.38        18\n",
      "           5       0.69      0.73      0.71        15\n",
      "\n",
      "    accuracy                           0.62        96\n",
      "   macro avg       0.63      0.62      0.61        96\n",
      "weighted avg       0.63      0.62      0.61        96\n",
      "\n",
      "Accuracy for Month 7: 0.625\n",
      "\n",
      "\n",
      "Month 8 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.95      0.64        19\n",
      "           2       0.57      0.17      0.27        23\n",
      "           3       0.59      0.68      0.63        28\n",
      "           4       0.50      0.27      0.35        11\n",
      "           5       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.58        96\n",
      "   macro avg       0.60      0.57      0.54        96\n",
      "weighted avg       0.60      0.58      0.55        96\n",
      "\n",
      "Accuracy for Month 8: 0.5833333333333334\n",
      "\n",
      "\n",
      "Month 9 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.94      0.56        16\n",
      "           2       0.46      0.20      0.28        30\n",
      "           3       0.73      0.77      0.75        35\n",
      "           4       0.50      0.30      0.37        10\n",
      "           5       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.55        96\n",
      "   macro avg       0.62      0.52      0.51        96\n",
      "weighted avg       0.58      0.55      0.52        96\n",
      "\n",
      "Accuracy for Month 9: 0.5520833333333334\n",
      "\n",
      "\n",
      "Month 10 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.54      0.41        13\n",
      "           2       0.53      0.50      0.52        32\n",
      "           3       0.53      0.70      0.61        33\n",
      "           4       0.00      0.00      0.00        12\n",
      "           5       0.50      0.17      0.25         6\n",
      "\n",
      "    accuracy                           0.49        96\n",
      "   macro avg       0.38      0.38      0.36        96\n",
      "weighted avg       0.44      0.49      0.45        96\n",
      "\n",
      "Accuracy for Month 10: 0.4895833333333333\n",
      "\n",
      "\n",
      "Month 11 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.67      0.47        15\n",
      "           2       0.65      0.58      0.61        38\n",
      "           3       0.50      0.56      0.53        27\n",
      "           4       0.33      0.08      0.13        12\n",
      "           5       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.50        96\n",
      "   macro avg       0.37      0.38      0.35        96\n",
      "weighted avg       0.49      0.50      0.48        96\n",
      "\n",
      "Accuracy for Month 11: 0.5\n",
      "\n",
      "\n",
      "Month 12 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.95      0.77        43\n",
      "           2       0.38      0.18      0.24        28\n",
      "           3       0.47      0.50      0.48        16\n",
      "           4       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.56        96\n",
      "   macro avg       0.37      0.41      0.37        96\n",
      "weighted avg       0.48      0.56      0.50        96\n",
      "\n",
      "Accuracy for Month 12: 0.5625\n",
      "\n",
      "\n",
      "Average Accuracy Across All Months: 0.5199652777777777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('../Data/training.csv')\n",
    "df_test = pd.read_csv('../Data/testing.csv')\n",
    "df_reference = pd.read_csv('../Data/reference.csv')\n",
    "\n",
    "X_train = df.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "y_train = df[[f'f{i}' for i in range(1, 13)]]\n",
    "X_test = df_test.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "y_test = df_reference[[f'f{i}' for i in range(1, 13)]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report for each month:\\n\")\n",
    "overall_accuracy = 0\n",
    "for i in range(1, 13):\n",
    "    report = classification_report(y_test[f'f{i}'], predictions[:, i-1], zero_division=0)\n",
    "    print(f\"\\nMonth {i} Report:\\n\")\n",
    "    print(report)\n",
    "    accuracy = accuracy_score(y_test[f'f{i}'], predictions[:, i-1])\n",
    "    overall_accuracy += accuracy\n",
    "    print(f\"Accuracy for Month {i}: {accuracy}\\n\")\n",
    "\n",
    "overall_accuracy /= 12\n",
    "print(f\"\\nAverage Accuracy Across All Months: {overall_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.3340463638305664\n",
      "Epoch 2/500, Loss: 1.8379535675048828\n",
      "Epoch 3/500, Loss: 1.0806798934936523\n",
      "Epoch 4/500, Loss: 0.9715931415557861\n",
      "Epoch 5/500, Loss: 0.8617820143699646\n",
      "Epoch 6/500, Loss: 0.6207723021507263\n",
      "Epoch 7/500, Loss: 0.7116687893867493\n",
      "Epoch 8/500, Loss: 0.6608386039733887\n",
      "Epoch 9/500, Loss: 0.5642830729484558\n",
      "Epoch 10/500, Loss: 0.7522931694984436\n",
      "Epoch 11/500, Loss: 0.4116623103618622\n",
      "Epoch 12/500, Loss: 0.5695436000823975\n",
      "Epoch 13/500, Loss: 0.6837509274482727\n",
      "Epoch 14/500, Loss: 0.5593960285186768\n",
      "Epoch 15/500, Loss: 0.39567992091178894\n",
      "Epoch 16/500, Loss: 0.4457952082157135\n",
      "Epoch 17/500, Loss: 0.41770198941230774\n",
      "Epoch 18/500, Loss: 0.28929516673088074\n",
      "Epoch 19/500, Loss: 0.2860819399356842\n",
      "Epoch 20/500, Loss: 0.3534599840641022\n",
      "Epoch 21/500, Loss: 0.3714633882045746\n",
      "Epoch 22/500, Loss: 0.3956204652786255\n",
      "Epoch 23/500, Loss: 0.28042787313461304\n",
      "Epoch 24/500, Loss: 0.26742884516716003\n",
      "Epoch 25/500, Loss: 0.3374136686325073\n",
      "Epoch 26/500, Loss: 0.3419041335582733\n",
      "Epoch 27/500, Loss: 0.2215268760919571\n",
      "Epoch 28/500, Loss: 0.26711007952690125\n",
      "Epoch 29/500, Loss: 0.2171909362077713\n",
      "Epoch 30/500, Loss: 0.19650046527385712\n",
      "Epoch 31/500, Loss: 0.24330385029315948\n",
      "Epoch 32/500, Loss: 0.2913168668746948\n",
      "Epoch 33/500, Loss: 0.20608766376972198\n",
      "Epoch 34/500, Loss: 0.21913887560367584\n",
      "Epoch 35/500, Loss: 0.17488567531108856\n",
      "Epoch 36/500, Loss: 0.21669413149356842\n",
      "Epoch 37/500, Loss: 0.23699773848056793\n",
      "Epoch 38/500, Loss: 0.24558138847351074\n",
      "Epoch 39/500, Loss: 0.22686533629894257\n",
      "Epoch 40/500, Loss: 0.20038527250289917\n",
      "Epoch 41/500, Loss: 0.19339723885059357\n",
      "Epoch 42/500, Loss: 0.2881658375263214\n",
      "Epoch 43/500, Loss: 0.2873033881187439\n",
      "Epoch 44/500, Loss: 0.2263309210538864\n",
      "Epoch 45/500, Loss: 0.17332422733306885\n",
      "Epoch 46/500, Loss: 0.1755513697862625\n",
      "Epoch 47/500, Loss: 0.15986043214797974\n",
      "Epoch 48/500, Loss: 0.14093515276908875\n",
      "Epoch 49/500, Loss: 0.19367122650146484\n",
      "Epoch 50/500, Loss: 0.174444779753685\n",
      "Epoch 51/500, Loss: 0.1925947219133377\n",
      "Epoch 52/500, Loss: 0.12359979003667831\n",
      "Epoch 53/500, Loss: 0.15379035472869873\n",
      "Epoch 54/500, Loss: 0.20653432607650757\n",
      "Epoch 55/500, Loss: 0.16679835319519043\n",
      "Epoch 56/500, Loss: 0.15684808790683746\n",
      "Epoch 57/500, Loss: 0.16139452159404755\n",
      "Epoch 58/500, Loss: 0.14320030808448792\n",
      "Epoch 59/500, Loss: 0.12011732906103134\n",
      "Epoch 60/500, Loss: 0.11668141931295395\n",
      "Epoch 61/500, Loss: 0.1263408362865448\n",
      "Epoch 62/500, Loss: 0.11438822001218796\n",
      "Epoch 63/500, Loss: 0.1154244914650917\n",
      "Epoch 64/500, Loss: 0.10678795725107193\n",
      "Epoch 65/500, Loss: 0.11877099424600601\n",
      "Epoch 66/500, Loss: 0.09331580251455307\n",
      "Epoch 67/500, Loss: 0.11004737764596939\n",
      "Epoch 68/500, Loss: 0.10704201459884644\n",
      "Epoch 69/500, Loss: 0.085441954433918\n",
      "Epoch 70/500, Loss: 0.12044268101453781\n",
      "Epoch 71/500, Loss: 0.07957644015550613\n",
      "Epoch 72/500, Loss: 0.10306239873170853\n",
      "Epoch 73/500, Loss: 0.09675600379705429\n",
      "Epoch 74/500, Loss: 0.09498365968465805\n",
      "Epoch 75/500, Loss: 0.08200051635503769\n",
      "Epoch 76/500, Loss: 0.10666555166244507\n",
      "Epoch 77/500, Loss: 0.08527705818414688\n",
      "Epoch 78/500, Loss: 0.07815210521221161\n",
      "Epoch 79/500, Loss: 0.07240509241819382\n",
      "Epoch 80/500, Loss: 0.09154081344604492\n",
      "Epoch 81/500, Loss: 0.08382681757211685\n",
      "Epoch 82/500, Loss: 0.09441816806793213\n",
      "Epoch 83/500, Loss: 0.11040014028549194\n",
      "Epoch 84/500, Loss: 0.1066863164305687\n",
      "Epoch 85/500, Loss: 0.09979496151208878\n",
      "Epoch 86/500, Loss: 0.1018524169921875\n",
      "Epoch 87/500, Loss: 0.08434086292982101\n",
      "Epoch 88/500, Loss: 0.07878757268190384\n",
      "Epoch 89/500, Loss: 0.1579122096300125\n",
      "Epoch 90/500, Loss: 0.1468198150396347\n",
      "Epoch 91/500, Loss: 0.15147334337234497\n",
      "Epoch 92/500, Loss: 0.14077503979206085\n",
      "Epoch 93/500, Loss: 0.18476760387420654\n",
      "Epoch 94/500, Loss: 0.18223434686660767\n",
      "Epoch 95/500, Loss: 0.14415772259235382\n",
      "Epoch 96/500, Loss: 0.10993918031454086\n",
      "Epoch 97/500, Loss: 0.09543930739164352\n",
      "Epoch 98/500, Loss: 0.12144589424133301\n",
      "Epoch 99/500, Loss: 0.11245381832122803\n",
      "Epoch 100/500, Loss: 0.1175084337592125\n",
      "Epoch 101/500, Loss: 0.08912983536720276\n",
      "Epoch 102/500, Loss: 0.07700613141059875\n",
      "Epoch 103/500, Loss: 0.06766519695520401\n",
      "Epoch 104/500, Loss: 0.09446962922811508\n",
      "Epoch 105/500, Loss: 0.10027202218770981\n",
      "Epoch 106/500, Loss: 0.07768823206424713\n",
      "Epoch 107/500, Loss: 0.07563067972660065\n",
      "Epoch 108/500, Loss: 0.06067020073533058\n",
      "Epoch 109/500, Loss: 0.09759069234132767\n",
      "Epoch 110/500, Loss: 0.08894038945436478\n",
      "Epoch 111/500, Loss: 0.08419812470674515\n",
      "Epoch 112/500, Loss: 0.0836937427520752\n",
      "Epoch 113/500, Loss: 0.10821643471717834\n",
      "Epoch 114/500, Loss: 0.09771756082773209\n",
      "Epoch 115/500, Loss: 0.06731816381216049\n",
      "Epoch 116/500, Loss: 0.1309986263513565\n",
      "Epoch 117/500, Loss: 0.1060103178024292\n",
      "Epoch 118/500, Loss: 0.12453553825616837\n",
      "Epoch 119/500, Loss: 0.10879331082105637\n",
      "Epoch 120/500, Loss: 0.06627625972032547\n",
      "Epoch 121/500, Loss: 0.08887485414743423\n",
      "Epoch 122/500, Loss: 0.0954538881778717\n",
      "Epoch 123/500, Loss: 0.06310230493545532\n",
      "Epoch 124/500, Loss: 0.06738115102052689\n",
      "Epoch 125/500, Loss: 0.0592622309923172\n",
      "Epoch 126/500, Loss: 0.06250787526369095\n",
      "Epoch 127/500, Loss: 0.050636451691389084\n",
      "Epoch 128/500, Loss: 0.050495367497205734\n",
      "Epoch 129/500, Loss: 0.043758880347013474\n",
      "Epoch 130/500, Loss: 0.06908146291971207\n",
      "Epoch 131/500, Loss: 0.04537701979279518\n",
      "Epoch 132/500, Loss: 0.05467827618122101\n",
      "Epoch 133/500, Loss: 0.047132477164268494\n",
      "Epoch 134/500, Loss: 0.042509909719228745\n",
      "Epoch 135/500, Loss: 0.054069697856903076\n",
      "Epoch 136/500, Loss: 0.05854812264442444\n",
      "Epoch 137/500, Loss: 0.054394762963056564\n",
      "Epoch 138/500, Loss: 0.05270959064364433\n",
      "Epoch 139/500, Loss: 0.04297938570380211\n",
      "Epoch 140/500, Loss: 0.04225407540798187\n",
      "Epoch 141/500, Loss: 0.048314642161130905\n",
      "Epoch 142/500, Loss: 0.046253759413957596\n",
      "Epoch 143/500, Loss: 0.0335017554461956\n",
      "Epoch 144/500, Loss: 0.054813727736473083\n",
      "Epoch 145/500, Loss: 0.06136820837855339\n",
      "Epoch 146/500, Loss: 0.06039049103856087\n",
      "Epoch 147/500, Loss: 0.04911120608448982\n",
      "Epoch 148/500, Loss: 0.0528826005756855\n",
      "Epoch 149/500, Loss: 0.05071519687771797\n",
      "Epoch 150/500, Loss: 0.05939241126179695\n",
      "Epoch 151/500, Loss: 0.04547214135527611\n",
      "Epoch 152/500, Loss: 0.040823135524988174\n",
      "Epoch 153/500, Loss: 0.037549350410699844\n",
      "Epoch 154/500, Loss: 0.054715346544981\n",
      "Epoch 155/500, Loss: 0.04874936863780022\n",
      "Epoch 156/500, Loss: 0.04260404780507088\n",
      "Epoch 157/500, Loss: 0.03885459899902344\n",
      "Epoch 158/500, Loss: 0.04457614943385124\n",
      "Epoch 159/500, Loss: 0.048934269696474075\n",
      "Epoch 160/500, Loss: 0.03186967223882675\n",
      "Epoch 161/500, Loss: 0.03144511207938194\n",
      "Epoch 162/500, Loss: 0.02554783970117569\n",
      "Epoch 163/500, Loss: 0.03174158185720444\n",
      "Epoch 164/500, Loss: 0.03249778226017952\n",
      "Epoch 165/500, Loss: 0.030111117288470268\n",
      "Epoch 166/500, Loss: 0.03301643952727318\n",
      "Epoch 167/500, Loss: 0.02618422918021679\n",
      "Epoch 168/500, Loss: 0.026660919189453125\n",
      "Epoch 169/500, Loss: 0.02504207193851471\n",
      "Epoch 170/500, Loss: 0.02625390887260437\n",
      "Epoch 171/500, Loss: 0.02607758902013302\n",
      "Epoch 172/500, Loss: 0.025357745587825775\n",
      "Epoch 173/500, Loss: 0.021698549389839172\n",
      "Epoch 174/500, Loss: 0.019260531291365623\n",
      "Epoch 175/500, Loss: 0.021975545212626457\n",
      "Epoch 176/500, Loss: 0.021879831328988075\n",
      "Epoch 177/500, Loss: 0.02661205641925335\n",
      "Epoch 178/500, Loss: 0.02809825725853443\n",
      "Epoch 179/500, Loss: 0.030951961874961853\n",
      "Epoch 180/500, Loss: 0.02767997421324253\n",
      "Epoch 181/500, Loss: 0.02647840417921543\n",
      "Epoch 182/500, Loss: 0.039012592285871506\n",
      "Epoch 183/500, Loss: 0.028740452602505684\n",
      "Epoch 184/500, Loss: 0.03789161145687103\n",
      "Epoch 185/500, Loss: 0.0245696809142828\n",
      "Epoch 186/500, Loss: 0.03172238543629646\n",
      "Epoch 187/500, Loss: 0.03214699774980545\n",
      "Epoch 188/500, Loss: 0.04454442486166954\n",
      "Epoch 189/500, Loss: 0.03487236052751541\n",
      "Epoch 190/500, Loss: 0.044737666845321655\n",
      "Epoch 191/500, Loss: 0.03689826652407646\n",
      "Epoch 192/500, Loss: 0.04131985083222389\n",
      "Epoch 193/500, Loss: 0.03425595909357071\n",
      "Epoch 194/500, Loss: 0.03132074326276779\n",
      "Epoch 195/500, Loss: 0.029838407412171364\n",
      "Epoch 196/500, Loss: 0.03313305601477623\n",
      "Epoch 197/500, Loss: 0.04162032529711723\n",
      "Epoch 198/500, Loss: 0.04026830196380615\n",
      "Epoch 199/500, Loss: 0.029962316155433655\n",
      "Epoch 200/500, Loss: 0.022619856521487236\n",
      "Epoch 201/500, Loss: 0.020885253325104713\n",
      "Epoch 202/500, Loss: 0.02415555715560913\n",
      "Epoch 203/500, Loss: 0.0233621746301651\n",
      "Epoch 204/500, Loss: 0.017716407775878906\n",
      "Epoch 205/500, Loss: 0.019364098086953163\n",
      "Epoch 206/500, Loss: 0.02193235419690609\n",
      "Epoch 207/500, Loss: 0.020761417225003242\n",
      "Epoch 208/500, Loss: 0.025099316611886024\n",
      "Epoch 209/500, Loss: 0.018149355426430702\n",
      "Epoch 210/500, Loss: 0.03308336064219475\n",
      "Epoch 211/500, Loss: 0.02303423546254635\n",
      "Epoch 212/500, Loss: 0.027924055233597755\n",
      "Epoch 213/500, Loss: 0.026936611160635948\n",
      "Epoch 214/500, Loss: 0.024365708231925964\n",
      "Epoch 215/500, Loss: 0.024414852261543274\n",
      "Epoch 216/500, Loss: 0.031358521431684494\n",
      "Epoch 217/500, Loss: 0.02443382702767849\n",
      "Epoch 218/500, Loss: 0.02439400553703308\n",
      "Epoch 219/500, Loss: 0.024743182584643364\n",
      "Epoch 220/500, Loss: 0.02166846953332424\n",
      "Epoch 221/500, Loss: 0.016285091638565063\n",
      "Epoch 222/500, Loss: 0.02578379213809967\n",
      "Epoch 223/500, Loss: 0.0199880488216877\n",
      "Epoch 224/500, Loss: 0.021005956456065178\n",
      "Epoch 225/500, Loss: 0.022574253380298615\n",
      "Epoch 226/500, Loss: 0.02741117589175701\n",
      "Epoch 227/500, Loss: 0.033493343740701675\n",
      "Epoch 228/500, Loss: 0.04250161722302437\n",
      "Epoch 229/500, Loss: 0.03543466329574585\n",
      "Epoch 230/500, Loss: 0.03492821380496025\n",
      "Epoch 231/500, Loss: 0.03209186717867851\n",
      "Epoch 232/500, Loss: 0.03210237994790077\n",
      "Epoch 233/500, Loss: 0.03463639318943024\n",
      "Epoch 234/500, Loss: 0.05053282156586647\n",
      "Epoch 235/500, Loss: 0.056864093989133835\n",
      "Epoch 236/500, Loss: 0.05074700713157654\n",
      "Epoch 237/500, Loss: 0.04108104854822159\n",
      "Epoch 238/500, Loss: 0.03734079375863075\n",
      "Epoch 239/500, Loss: 0.041906457394361496\n",
      "Epoch 240/500, Loss: 0.03457305207848549\n",
      "Epoch 241/500, Loss: 0.03766973689198494\n",
      "Epoch 242/500, Loss: 0.03430367633700371\n",
      "Epoch 243/500, Loss: 0.03974999487400055\n",
      "Epoch 244/500, Loss: 0.02392580546438694\n",
      "Epoch 245/500, Loss: 0.027685238048434258\n",
      "Epoch 246/500, Loss: 0.024681001901626587\n",
      "Epoch 247/500, Loss: 0.023840785026550293\n",
      "Epoch 248/500, Loss: 0.030526237562298775\n",
      "Epoch 249/500, Loss: 0.027115970849990845\n",
      "Epoch 250/500, Loss: 0.0198697317391634\n",
      "Epoch 251/500, Loss: 0.014053280465304852\n",
      "Epoch 252/500, Loss: 0.01628916896879673\n",
      "Epoch 253/500, Loss: 0.018021348863840103\n",
      "Epoch 254/500, Loss: 0.013597160577774048\n",
      "Epoch 255/500, Loss: 0.016214659437537193\n",
      "Epoch 256/500, Loss: 0.01659872941672802\n",
      "Epoch 257/500, Loss: 0.017932990565896034\n",
      "Epoch 258/500, Loss: 0.020931893959641457\n",
      "Epoch 259/500, Loss: 0.01817333698272705\n",
      "Epoch 260/500, Loss: 0.016039343550801277\n",
      "Epoch 261/500, Loss: 0.017314188182353973\n",
      "Epoch 262/500, Loss: 0.020867710933089256\n",
      "Epoch 263/500, Loss: 0.0181142408400774\n",
      "Epoch 264/500, Loss: 0.029312213882803917\n",
      "Epoch 265/500, Loss: 0.025862528011202812\n",
      "Epoch 266/500, Loss: 0.02515924721956253\n",
      "Epoch 267/500, Loss: 0.02561175636947155\n",
      "Epoch 268/500, Loss: 0.022830119356513023\n",
      "Epoch 269/500, Loss: 0.025243423879146576\n",
      "Epoch 270/500, Loss: 0.029498914256691933\n",
      "Epoch 271/500, Loss: 0.030360160395503044\n",
      "Epoch 272/500, Loss: 0.0340907908976078\n",
      "Epoch 273/500, Loss: 0.027153387665748596\n",
      "Epoch 274/500, Loss: 0.0316288135945797\n",
      "Epoch 275/500, Loss: 0.05026821792125702\n",
      "Epoch 276/500, Loss: 0.036100272089242935\n",
      "Epoch 277/500, Loss: 0.04570756480097771\n",
      "Epoch 278/500, Loss: 0.0379006527364254\n",
      "Epoch 279/500, Loss: 0.04550333321094513\n",
      "Epoch 280/500, Loss: 0.04428022727370262\n",
      "Epoch 281/500, Loss: 0.04918317869305611\n",
      "Epoch 282/500, Loss: 0.04341074824333191\n",
      "Epoch 283/500, Loss: 0.04671667143702507\n",
      "Epoch 284/500, Loss: 0.06660971790552139\n",
      "Epoch 285/500, Loss: 0.04669347032904625\n",
      "Epoch 286/500, Loss: 0.06077676638960838\n",
      "Epoch 287/500, Loss: 0.04519358277320862\n",
      "Epoch 288/500, Loss: 0.06524055451154709\n",
      "Epoch 289/500, Loss: 0.05969616770744324\n",
      "Epoch 290/500, Loss: 0.05418439581990242\n",
      "Epoch 291/500, Loss: 0.05043600872159004\n",
      "Epoch 292/500, Loss: 0.03732747957110405\n",
      "Epoch 293/500, Loss: 0.03826473280787468\n",
      "Epoch 294/500, Loss: 0.049686502665281296\n",
      "Epoch 295/500, Loss: 0.035042185336351395\n",
      "Epoch 296/500, Loss: 0.03374938666820526\n",
      "Epoch 297/500, Loss: 0.03470280393958092\n",
      "Epoch 298/500, Loss: 0.037618886679410934\n",
      "Epoch 299/500, Loss: 0.029173582792282104\n",
      "Epoch 300/500, Loss: 0.023655543103814125\n",
      "Epoch 301/500, Loss: 0.03249392658472061\n",
      "Epoch 302/500, Loss: 0.03218597173690796\n",
      "Epoch 303/500, Loss: 0.0516781359910965\n",
      "Epoch 304/500, Loss: 0.03937749192118645\n",
      "Epoch 305/500, Loss: 0.03069019876420498\n",
      "Epoch 306/500, Loss: 0.023735469207167625\n",
      "Epoch 307/500, Loss: 0.029187122359871864\n",
      "Epoch 308/500, Loss: 0.03293187543749809\n",
      "Epoch 309/500, Loss: 0.032066673040390015\n",
      "Epoch 310/500, Loss: 0.03994595631957054\n",
      "Epoch 311/500, Loss: 0.025583820417523384\n",
      "Epoch 312/500, Loss: 0.02206755429506302\n",
      "Epoch 313/500, Loss: 0.021374480798840523\n",
      "Epoch 314/500, Loss: 0.027025355026125908\n",
      "Epoch 315/500, Loss: 0.02643769048154354\n",
      "Epoch 316/500, Loss: 0.0341949425637722\n",
      "Epoch 317/500, Loss: 0.029711326584219933\n",
      "Epoch 318/500, Loss: 0.031264033168554306\n",
      "Epoch 319/500, Loss: 0.03859291970729828\n",
      "Epoch 320/500, Loss: 0.026661531999707222\n",
      "Epoch 321/500, Loss: 0.047566935420036316\n",
      "Epoch 322/500, Loss: 0.029752159491181374\n",
      "Epoch 323/500, Loss: 0.025275209918618202\n",
      "Epoch 324/500, Loss: 0.02999436855316162\n",
      "Epoch 325/500, Loss: 0.02313581295311451\n",
      "Epoch 326/500, Loss: 0.021074892953038216\n",
      "Epoch 327/500, Loss: 0.02340819127857685\n",
      "Epoch 328/500, Loss: 0.016883408650755882\n",
      "Epoch 329/500, Loss: 0.02284456603229046\n",
      "Epoch 330/500, Loss: 0.02445240318775177\n",
      "Epoch 331/500, Loss: 0.015195623971521854\n",
      "Epoch 332/500, Loss: 0.02153187058866024\n",
      "Epoch 333/500, Loss: 0.02485550008714199\n",
      "Epoch 334/500, Loss: 0.02106202207505703\n",
      "Epoch 335/500, Loss: 0.02642192132771015\n",
      "Epoch 336/500, Loss: 0.01692528836429119\n",
      "Epoch 337/500, Loss: 0.049138303846120834\n",
      "Epoch 338/500, Loss: 0.06087452545762062\n",
      "Epoch 339/500, Loss: 0.025973863899707794\n",
      "Epoch 340/500, Loss: 0.04121560975909233\n",
      "Epoch 341/500, Loss: 0.043317969888448715\n",
      "Epoch 342/500, Loss: 0.04458431527018547\n",
      "Epoch 343/500, Loss: 0.06596048921346664\n",
      "Epoch 344/500, Loss: 0.05349789187312126\n",
      "Epoch 345/500, Loss: 0.04183397814631462\n",
      "Epoch 346/500, Loss: 0.057259827852249146\n",
      "Epoch 347/500, Loss: 0.044162485748529434\n",
      "Epoch 348/500, Loss: 0.04392001032829285\n",
      "Epoch 349/500, Loss: 0.05169796943664551\n",
      "Epoch 350/500, Loss: 0.04022900387644768\n",
      "Epoch 351/500, Loss: 0.049096446484327316\n",
      "Epoch 352/500, Loss: 0.032480839639902115\n",
      "Epoch 353/500, Loss: 0.04015958681702614\n",
      "Epoch 354/500, Loss: 0.02758215367794037\n",
      "Epoch 355/500, Loss: 0.03785945847630501\n",
      "Epoch 356/500, Loss: 0.02967027761042118\n",
      "Epoch 357/500, Loss: 0.04420146346092224\n",
      "Epoch 358/500, Loss: 0.02734220027923584\n",
      "Epoch 359/500, Loss: 0.02083590067923069\n",
      "Epoch 360/500, Loss: 0.03294159844517708\n",
      "Epoch 361/500, Loss: 0.029867229983210564\n",
      "Epoch 362/500, Loss: 0.03356878086924553\n",
      "Epoch 363/500, Loss: 0.028087066486477852\n",
      "Epoch 364/500, Loss: 0.030505292117595673\n",
      "Epoch 365/500, Loss: 0.05900266766548157\n",
      "Epoch 366/500, Loss: 0.039709318429231644\n",
      "Epoch 367/500, Loss: 0.03323793783783913\n",
      "Epoch 368/500, Loss: 0.0364852249622345\n",
      "Epoch 369/500, Loss: 0.03412690386176109\n",
      "Epoch 370/500, Loss: 0.02977895736694336\n",
      "Epoch 371/500, Loss: 0.02652657963335514\n",
      "Epoch 372/500, Loss: 0.03778940811753273\n",
      "Epoch 373/500, Loss: 0.02138322778046131\n",
      "Epoch 374/500, Loss: 0.035571493208408356\n",
      "Epoch 375/500, Loss: 0.01788928732275963\n",
      "Epoch 376/500, Loss: 0.03928272798657417\n",
      "Epoch 377/500, Loss: 0.026223933324217796\n",
      "Epoch 378/500, Loss: 0.029882369562983513\n",
      "Epoch 379/500, Loss: 0.02370489202439785\n",
      "Epoch 380/500, Loss: 0.0280862245708704\n",
      "Epoch 381/500, Loss: 0.021714255213737488\n",
      "Epoch 382/500, Loss: 0.022969981655478477\n",
      "Epoch 383/500, Loss: 0.022244691848754883\n",
      "Epoch 384/500, Loss: 0.02005966193974018\n",
      "Epoch 385/500, Loss: 0.018254488706588745\n",
      "Epoch 386/500, Loss: 0.025415904819965363\n",
      "Epoch 387/500, Loss: 0.01539504062384367\n",
      "Epoch 388/500, Loss: 0.021019062027335167\n",
      "Epoch 389/500, Loss: 0.025730876252055168\n",
      "Epoch 390/500, Loss: 0.01744988188147545\n",
      "Epoch 391/500, Loss: 0.014730674214661121\n",
      "Epoch 392/500, Loss: 0.013036531396210194\n",
      "Epoch 393/500, Loss: 0.015815436840057373\n",
      "Epoch 394/500, Loss: 0.00985476840287447\n",
      "Epoch 395/500, Loss: 0.011878523975610733\n",
      "Epoch 396/500, Loss: 0.01121861394494772\n",
      "Epoch 397/500, Loss: 0.009521410800516605\n",
      "Epoch 398/500, Loss: 0.009759954176843166\n",
      "Epoch 399/500, Loss: 0.00894131138920784\n",
      "Epoch 400/500, Loss: 0.00887975562363863\n",
      "Epoch 401/500, Loss: 0.007629712577909231\n",
      "Epoch 402/500, Loss: 0.010291628539562225\n",
      "Epoch 403/500, Loss: 0.012710089795291424\n",
      "Epoch 404/500, Loss: 0.011164340190589428\n",
      "Epoch 405/500, Loss: 0.00958091951906681\n",
      "Epoch 406/500, Loss: 0.012962584383785725\n",
      "Epoch 407/500, Loss: 0.014900055713951588\n",
      "Epoch 408/500, Loss: 0.011895337142050266\n",
      "Epoch 409/500, Loss: 0.01230130810290575\n",
      "Epoch 410/500, Loss: 0.011738852597773075\n",
      "Epoch 411/500, Loss: 0.01327244471758604\n",
      "Epoch 412/500, Loss: 0.014038628898561\n",
      "Epoch 413/500, Loss: 0.015388105995953083\n",
      "Epoch 414/500, Loss: 0.013476821593940258\n",
      "Epoch 415/500, Loss: 0.013591458089649677\n",
      "Epoch 416/500, Loss: 0.022355379536747932\n",
      "Epoch 417/500, Loss: 0.014521480537950993\n",
      "Epoch 418/500, Loss: 0.016567662358283997\n",
      "Epoch 419/500, Loss: 0.0579519160091877\n",
      "Epoch 420/500, Loss: 0.03950266167521477\n",
      "Epoch 421/500, Loss: 0.03479321673512459\n",
      "Epoch 422/500, Loss: 0.033444151282310486\n",
      "Epoch 423/500, Loss: 0.04093353822827339\n",
      "Epoch 424/500, Loss: 0.0344131737947464\n",
      "Epoch 425/500, Loss: 0.030374472960829735\n",
      "Epoch 426/500, Loss: 0.04230393096804619\n",
      "Epoch 427/500, Loss: 0.038309600204229355\n",
      "Epoch 428/500, Loss: 0.025989817455410957\n",
      "Epoch 429/500, Loss: 0.029836321249604225\n",
      "Epoch 430/500, Loss: 0.023579778149724007\n",
      "Epoch 431/500, Loss: 0.0311757642775774\n",
      "Epoch 432/500, Loss: 0.02391974814236164\n",
      "Epoch 433/500, Loss: 0.024229807779192924\n",
      "Epoch 434/500, Loss: 0.030159926041960716\n",
      "Epoch 435/500, Loss: 0.01786619983613491\n",
      "Epoch 436/500, Loss: 0.021672045812010765\n",
      "Epoch 437/500, Loss: 0.028609970584511757\n",
      "Epoch 438/500, Loss: 0.020127585157752037\n",
      "Epoch 439/500, Loss: 0.018010174855589867\n",
      "Epoch 440/500, Loss: 0.015230407007038593\n",
      "Epoch 441/500, Loss: 0.016173774376511574\n",
      "Epoch 442/500, Loss: 0.011189949698746204\n",
      "Epoch 443/500, Loss: 0.010133528150618076\n",
      "Epoch 444/500, Loss: 0.009528150781989098\n",
      "Epoch 445/500, Loss: 0.013245227746665478\n",
      "Epoch 446/500, Loss: 0.015164284966886044\n",
      "Epoch 447/500, Loss: 0.012791299261152744\n",
      "Epoch 448/500, Loss: 0.01771407015621662\n",
      "Epoch 449/500, Loss: 0.015539966523647308\n",
      "Epoch 450/500, Loss: 0.021651260554790497\n",
      "Epoch 451/500, Loss: 0.017930375412106514\n",
      "Epoch 452/500, Loss: 0.02554631419479847\n",
      "Epoch 453/500, Loss: 0.021026914939284325\n",
      "Epoch 454/500, Loss: 0.024190843105316162\n",
      "Epoch 455/500, Loss: 0.02421974390745163\n",
      "Epoch 456/500, Loss: 0.02355138026177883\n",
      "Epoch 457/500, Loss: 0.0541740320622921\n",
      "Epoch 458/500, Loss: 0.03345553204417229\n",
      "Epoch 459/500, Loss: 0.02350224368274212\n",
      "Epoch 460/500, Loss: 0.0362403579056263\n",
      "Epoch 461/500, Loss: 0.03012520633637905\n",
      "Epoch 462/500, Loss: 0.033188220113515854\n",
      "Epoch 463/500, Loss: 0.021679455414414406\n",
      "Epoch 464/500, Loss: 0.03824077174067497\n",
      "Epoch 465/500, Loss: 0.03735232725739479\n",
      "Epoch 466/500, Loss: 0.033036720007658005\n",
      "Epoch 467/500, Loss: 0.03769565373659134\n",
      "Epoch 468/500, Loss: 0.02812008559703827\n",
      "Epoch 469/500, Loss: 0.029019435867667198\n",
      "Epoch 470/500, Loss: 0.04958757758140564\n",
      "Epoch 471/500, Loss: 0.024760114029049873\n",
      "Epoch 472/500, Loss: 0.03836850821971893\n",
      "Epoch 473/500, Loss: 0.03026493825018406\n",
      "Epoch 474/500, Loss: 0.031032821163535118\n",
      "Epoch 475/500, Loss: 0.04344351589679718\n",
      "Epoch 476/500, Loss: 0.019945675507187843\n",
      "Epoch 477/500, Loss: 0.03144460916519165\n",
      "Epoch 478/500, Loss: 0.021727563813328743\n",
      "Epoch 479/500, Loss: 0.017143813893198967\n",
      "Epoch 480/500, Loss: 0.016455648466944695\n",
      "Epoch 481/500, Loss: 0.020299894735217094\n",
      "Epoch 482/500, Loss: 0.014677200466394424\n",
      "Epoch 483/500, Loss: 0.0260931346565485\n",
      "Epoch 484/500, Loss: 0.022934451699256897\n",
      "Epoch 485/500, Loss: 0.022975193336606026\n",
      "Epoch 486/500, Loss: 0.03701026365160942\n",
      "Epoch 487/500, Loss: 0.03524536266922951\n",
      "Epoch 488/500, Loss: 0.029244698584079742\n",
      "Epoch 489/500, Loss: 0.02679910697042942\n",
      "Epoch 490/500, Loss: 0.030508698895573616\n",
      "Epoch 491/500, Loss: 0.03116973489522934\n",
      "Epoch 492/500, Loss: 0.03869886323809624\n",
      "Epoch 493/500, Loss: 0.037854667752981186\n",
      "Epoch 494/500, Loss: 0.02982831560075283\n",
      "Epoch 495/500, Loss: 0.024039849638938904\n",
      "Epoch 496/500, Loss: 0.02600477822124958\n",
      "Epoch 497/500, Loss: 0.0238050427287817\n",
      "Epoch 498/500, Loss: 0.02414274960756302\n",
      "Epoch 499/500, Loss: 0.01759526878595352\n",
      "Epoch 500/500, Loss: 0.022099539637565613\n",
      "Average Testing Loss: 0.977098286151886\n",
      "Monthly Accuracy:\n",
      "\n",
      "Month 1: 0.47\n",
      "\n",
      "Month 2: 0.48\n",
      "\n",
      "Month 3: 0.40\n",
      "\n",
      "Month 4: 0.35\n",
      "\n",
      "Month 5: 0.39\n",
      "\n",
      "Month 6: 0.47\n",
      "\n",
      "Month 7: 0.43\n",
      "\n",
      "Month 8: 0.38\n",
      "\n",
      "Month 9: 0.47\n",
      "\n",
      "Month 10: 0.39\n",
      "\n",
      "Month 11: 0.40\n",
      "\n",
      "Month 12: 0.45\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Network Model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    features = df.drop(columns=[f'f{i}' for i in range(1, 13)]).values\n",
    "    targets = df[[f'f{i}' for i in range(1, 13)]].values\n",
    "    return features, targets\n",
    "\n",
    "# Prepare Data\n",
    "scaler = StandardScaler()\n",
    "X_train, y_train = load_data('../Data/training.csv')\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test, y_test = load_data('../Data/reference.csv') #testing\n",
    "X_test = scaler.transform(X_test)\n",
    "y_reference = pd.read_csv('../Data/reference.csv')[[f'f{i}' for i in range(1, 13)]].values\n",
    "\n",
    "train_dataset = CustomDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = CustomDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "model = RegressionNN(input_size, output_size)\n",
    "losses = []\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0055)\n",
    "\n",
    "# Train Model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    for features, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate Model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        # Round the outputs to the nearest integer\n",
    "        rounded_outputs = torch.round(outputs).numpy()\n",
    "        predictions.extend(rounded_outputs)\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f'Average Testing Loss: {avg_loss}')\n",
    "\n",
    "# Compare with Reference and Calculate Accuracy\n",
    "accuracy_list = []\n",
    "for i in range(12):\n",
    "    correct = sum(1 for j in range(len(predictions)) if predictions[j][i] == y_reference[j][i])\n",
    "    accuracy = correct / len(predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "# Save results\n",
    "print('Monthly Accuracy:\\n')\n",
    "for i, acc in enumerate(accuracy_list, 1):\n",
    "    print(f'Month {i}: {acc:.2f}\\n')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title('Training and Testing Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Report for each month:\n",
      "\n",
      "\n",
      "Month 1 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.88      0.60        33\n",
      "           2       0.40      0.15      0.22        27\n",
      "           3       0.20      0.17      0.18        12\n",
      "           4       0.45      0.31      0.37        16\n",
      "           5       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.42        96\n",
      "   macro avg       0.30      0.30      0.27        96\n",
      "weighted avg       0.37      0.42      0.35        96\n",
      "\n",
      "Accuracy for Month 1: 0.4166666666666667\n",
      "\n",
      "\n",
      "Month 2 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.74      0.59        31\n",
      "           2       0.45      0.35      0.39        26\n",
      "           3       0.46      0.46      0.46        13\n",
      "           4       0.67      0.12      0.20        17\n",
      "           5       0.54      0.78      0.64         9\n",
      "\n",
      "    accuracy                           0.49        96\n",
      "   macro avg       0.52      0.49      0.46        96\n",
      "weighted avg       0.51      0.49      0.45        96\n",
      "\n",
      "Accuracy for Month 2: 0.4895833333333333\n",
      "\n",
      "\n",
      "Month 3 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.40      0.50        15\n",
      "           2       0.42      0.74      0.54        19\n",
      "           3       0.26      0.36      0.30        22\n",
      "           4       0.40      0.07      0.12        27\n",
      "           5       0.44      0.62      0.52        13\n",
      "\n",
      "    accuracy                           0.40        96\n",
      "   macro avg       0.44      0.44      0.40        96\n",
      "weighted avg       0.42      0.40      0.36        96\n",
      "\n",
      "Accuracy for Month 3: 0.3958333333333333\n",
      "\n",
      "\n",
      "Month 4 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.32      0.43      0.36        14\n",
      "           3       0.53      0.43      0.48        37\n",
      "           4       0.21      0.16      0.18        25\n",
      "           5       0.22      0.24      0.23        17\n",
      "\n",
      "    accuracy                           0.31        96\n",
      "   macro avg       0.26      0.25      0.25        96\n",
      "weighted avg       0.35      0.31      0.32        96\n",
      "\n",
      "Accuracy for Month 4: 0.3125\n",
      "\n",
      "\n",
      "Month 5 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.15      0.38      0.21         8\n",
      "           2       0.23      0.33      0.27        15\n",
      "           3       0.66      0.54      0.59        50\n",
      "           4       0.30      0.25      0.27        12\n",
      "           5       1.00      0.27      0.43        11\n",
      "\n",
      "    accuracy                           0.43        96\n",
      "   macro avg       0.47      0.35      0.36        96\n",
      "weighted avg       0.54      0.43      0.45        96\n",
      "\n",
      "Accuracy for Month 5: 0.4270833333333333\n",
      "\n",
      "\n",
      "Month 6 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.86      0.45        14\n",
      "           2       0.61      0.29      0.39        38\n",
      "           3       0.55      0.52      0.54        21\n",
      "           4       0.47      0.57      0.52        14\n",
      "           5       1.00      0.22      0.36         9\n",
      "\n",
      "    accuracy                           0.46        96\n",
      "   macro avg       0.59      0.49      0.45        96\n",
      "weighted avg       0.57      0.46      0.45        96\n",
      "\n",
      "Accuracy for Month 6: 0.4583333333333333\n",
      "\n",
      "\n",
      "Month 7 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.80      0.59        20\n",
      "           2       0.38      0.30      0.33        20\n",
      "           3       0.50      0.39      0.44        23\n",
      "           4       0.45      0.28      0.34        18\n",
      "           5       0.65      0.73      0.69        15\n",
      "\n",
      "    accuracy                           0.49        96\n",
      "   macro avg       0.49      0.50      0.48        96\n",
      "weighted avg       0.48      0.49      0.47        96\n",
      "\n",
      "Accuracy for Month 7: 0.4895833333333333\n",
      "\n",
      "\n",
      "Month 8 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.89      0.55        19\n",
      "           2       0.20      0.09      0.12        23\n",
      "           3       0.50      0.36      0.42        28\n",
      "           4       0.23      0.27      0.25        11\n",
      "           5       0.90      0.60      0.72        15\n",
      "\n",
      "    accuracy                           0.43        96\n",
      "   macro avg       0.45      0.44      0.41        96\n",
      "weighted avg       0.44      0.43      0.40        96\n",
      "\n",
      "Accuracy for Month 8: 0.4270833333333333\n",
      "\n",
      "\n",
      "Month 9 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.62      0.37        16\n",
      "           2       0.45      0.17      0.24        30\n",
      "           3       0.70      0.66      0.68        35\n",
      "           4       0.33      0.40      0.36        10\n",
      "           5       0.50      0.20      0.29         5\n",
      "\n",
      "    accuracy                           0.45        96\n",
      "   macro avg       0.45      0.41      0.39        96\n",
      "weighted avg       0.50      0.45      0.44        96\n",
      "\n",
      "Accuracy for Month 9: 0.4479166666666667\n",
      "\n",
      "\n",
      "Month 10 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.54      0.38        13\n",
      "           2       0.47      0.50      0.48        32\n",
      "           3       0.55      0.52      0.53        33\n",
      "           4       0.67      0.17      0.27        12\n",
      "           5       0.50      0.33      0.40         6\n",
      "\n",
      "    accuracy                           0.46        96\n",
      "   macro avg       0.50      0.41      0.41        96\n",
      "weighted avg       0.50      0.46      0.45        96\n",
      "\n",
      "Accuracy for Month 10: 0.4583333333333333\n",
      "\n",
      "\n",
      "Month 11 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.40      0.32        15\n",
      "           2       0.53      0.47      0.50        38\n",
      "           3       0.39      0.44      0.41        27\n",
      "           4       0.25      0.17      0.20        12\n",
      "           5       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.40        96\n",
      "   macro avg       0.29      0.30      0.29        96\n",
      "weighted avg       0.39      0.40      0.39        96\n",
      "\n",
      "Accuracy for Month 11: 0.3958333333333333\n",
      "\n",
      "\n",
      "Month 12 Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.91      0.74        43\n",
      "           2       0.39      0.25      0.30        28\n",
      "           3       0.44      0.25      0.32        16\n",
      "           4       0.33      0.22      0.27         9\n",
      "\n",
      "    accuracy                           0.54        96\n",
      "   macro avg       0.45      0.41      0.41        96\n",
      "weighted avg       0.50      0.54      0.50        96\n",
      "\n",
      "Accuracy for Month 12: 0.5416666666666666\n",
      "\n",
      "\n",
      "Average Accuracy Across All Months: 0.4383680555555556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "train_df = pd.read_csv('../Data/training.csv')\n",
    "test_df = pd.read_csv('../Data/testing.csv')\n",
    "reference_df = pd.read_csv('../Data/reference.csv')\n",
    "\n",
    "X_train = train_df.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "y_train = train_df[[f'f{i}' for i in range(1, 13)]]\n",
    "X_test = test_df.drop(columns=[f'f{i}' for i in range(1, 13)])\n",
    "y_test = reference_df[[f'f{i}' for i in range(1, 13)]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"KNN Classification Report for each month:\\n\")\n",
    "overall_accuracy = 0\n",
    "for i in range(1, 13):\n",
    "    report = classification_report(y_test[f'f{i}'], predictions[:, i-1], zero_division=0)\n",
    "    print(f\"\\nMonth {i} Report:\\n\")\n",
    "    print(report)\n",
    "    accuracy = accuracy_score(y_test[f'f{i}'], predictions[:, i-1])\n",
    "    overall_accuracy += accuracy\n",
    "    print(f\"Accuracy for Month {i}: {accuracy}\\n\")\n",
    "\n",
    "overall_accuracy /= 12\n",
    "print(f\"\\nAverage Accuracy Across All Months: {overall_accuracy}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
